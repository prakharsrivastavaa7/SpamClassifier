The original data downloaded from kaggle had multiple unnecessary columns and duplicate rows. Therefore cleaning of the original data was done in jupyter notebook and the new dataset obtained was then stored in a new csv file which was created by me. Thenewly created dataset was then used for preprocessing and modeling.
For model training using DecisionTreeClassifier I used hyperparameter tuning to improve the precision value. RandomisedSearchCV is used to define hyperparameter values. A grid is defined where values are passed for max_depth, max_features, min_samples_leaf and criterion. This grid is then used and the best estimator is derived. The best estimator is used to make predictions by fitting it on the X and y components of dataset.
A new methodology of voting classifier is used in this project by me. A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.MultinomialNaivebayes, LogisticRegression, BenoulliNaiveBayes, GaussianNaiveBayes,ExtraTreeClassifier,GradientBoostingClassifier and XGBClassifier are the various models used by me for voting classifier. In classification, a hard voting ensemble involves summing the votes for crisp class labels from other models and predicting the class with the most votes. A soft voting ensemble involves summing the predicted probabilities for class labels and predicting the class label with the largest sum probability. I have used soft voting in the project